

Hi Lisa, Adrian, Brian, and Barbara,

Unfortunately, I cannot make the meeting this afternoon due to a conflict, but here's a status update for discussion.

The experiment we're currently working on is to explore whether our current architecture will support the goals for Rikolti, and more specifically, whether Spark can effectively map metadata from our source institutions into the UCLDC schema. We're currently exploring two issues with Spark:

    1. Spark does not natively read XML but we harvest a lot of XML, and so need a way to convert between XML to a Spark-readable format in a lossless way.
    2. Glue (effectively AWS-flavored Spark) infers a schema over the data it reads. The schema that Glue infers must be consistent across all records and collections using the same mapper to migrate our existing Harvester 1 mappers to Rikolti mappers.   

Brian's done a lot of work to address the first problem and has produced the Datel command line application to convert XML to json: https://github.com/tingletech/datel/, as we had discussed in our meeting last week. For our experiment, I was going to run OAC and OAI harvests using the Datel application to replace the xmltojson module for converting XML to JSON to ensure that the resulting metadata produced table schemas (and cross-table schemas) that were compatible with our mapper architecture, addressing problem 2.

This is what had to happen to accomplish this:

    1. Modify Brian's command line application into a python module for use with the existing fetcher implementation to replace the xmltojson module I had been using.
    2. Create JSON payloads for kicking off the metadata fetching process for all OAC collections and ContentDM OAI collections.
    3. Run the OAC and OAI fetchers across all collections.
    4. Run the Glue Crawler across all collections to produce one table (and schema) per collection.
    5. Analyze the resulting schemas for all like collections to ensure they are all compatible with each other, such that one OAC mapper and one ContentDM OAI mapper can be written for all OAC/ContentDM collections. 

Because Datel depends on the lxml module, which is not a pure python module and thus not easily deployable to AWS Lambda, I am running the fetcher in a development environment locally on my laptop in sequence, making the fetching process very slow. Currently, I am still at step 3: waiting for the fetcher to finish running for all OAC collections. I have not yet run a fetcher using Datel over the ContentDM collections.

Meanwhile, Brian has made several updates to the Datel application. If those updates are relevant to the current experiment, I will have to re-port Datel to a python module and re-run the fetching process. At this point, given the speed with which the Datel application is currently being updated, I think it will be faster to re-write the fetcher to save XML from OAC and OAI, and write a new component to run Datel over the saved XML. This will still involve re-running the fetching process over all OAC collections, which takes a while. Once this is done, I can start the Glue crawler and begin analysis on the inferred spark schemas.

Given that it's going to take a couple more days to get meaningful data, I suggest postponing our meeting this afternoon to later this week.

Thanks,

- Amy
